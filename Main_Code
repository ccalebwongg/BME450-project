## Initialize Data <---

import os
import torch
from torch.utils.data import Dataset
from PIL import Image
from torchvision import transforms

class UTKFaceDataset(Dataset):
    def __init__(self, image_dir, transform=None):
        self.image_dir = image_dir
        self.transform = transform
        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]
        self.data = []

        for filename in self.image_files:
            try:
                age, gender, _, _ = filename.split('_', 3)
                age = int(age)
                gender = int(gender)

                if age > 100:
                    age = 100  # Cap ages above 100

                age_group = age // 10  # Age groups: 0–10 -> group 0, 11–20 -> group 1, ..., 91–100 -> group 9
                age_group = min(age_group, 9)  # Cap group at 9

                self.data.append((filename, age, age_group, gender))
            except:
                continue  # Skip files with unexpected formatting

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        filename, age, age_group, gender = self.data[idx]
        img_path = os.path.join(self.image_dir, filename)
        image = Image.open(img_path).convert("RGB")

        if self.transform:
            image = self.transform(image)

        return image, torch.tensor(age, dtype=torch.float32), torch.tensor(age_group), torch.tensor(gender)

## Define Transforms and Dataloaders <---

from torch.utils.data import DataLoader
from torchvision import transforms

transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
])

#initialize dataset and dataloaders
dataset = UTKFaceDataset(image_dir="UTKFace", transform=transform)

train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)


## Define Neural Network <---

import torch.nn as nn
import torch.nn.functional as F

class AgePredictorNet(nn.Module):
    def __init__(self):
        super(AgePredictorNet, self).__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # (3, 128, 128) -> (32, 128, 128)
            nn.ReLU(),
            nn.MaxPool2d(2),                             # -> (32, 64, 64)

            nn.Conv2d(32, 64, kernel_size=3, padding=1), # -> (64, 64, 64)
            nn.ReLU(),
            nn.MaxPool2d(2),                             # -> (64, 32, 32)

            nn.Conv2d(64, 128, kernel_size=3, padding=1),# -> (128, 32, 32)
            nn.ReLU(),
            nn.MaxPool2d(2),                             # -> (128, 16, 16)
        )

        self.flatten = nn.Flatten()
        self.fc = nn.Sequential(
            nn.Linear(128 * 16 * 16, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
        )

        # Output heads
        self.regressor = nn.Linear(512, 1)      # Age prediction
        self.classifier = nn.Linear(512, 10)    # Age group classification (0–9)

    def forward(self, x):
        x = self.conv_layers(x)
        x = self.flatten(x)
        x = self.fc(x)

        age_pred = self.regressor(x)
        age_group_pred = self.classifier(x)

        return age_pred.squeeze(1), age_group_pred

## Training and Test Loops <---

import torch.optim as optim

model = AgePredictorNet()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

#get losses
regression_loss_fn = nn.MSELoss()
classification_loss_fn = nn.CrossEntropyLoss()

#optimizer
optimizer = optim.Adam(model.parameters(), lr=1e-3)

#trainng loop
def train_loop(dataloader, model, optimizer):
    model.train()
    total_loss = 0
    for batch in dataloader:
        images, ages, age_groups, _ = batch
        images = images.to(device)
        ages = ages.to(device)
        age_groups = age_groups.to(device)

        optimizer.zero_grad()
        age_preds, group_preds = model(images)

        loss_reg = regression_loss_fn(age_preds, ages)
        loss_cls = classification_loss_fn(group_preds, age_groups)
        loss = loss_reg + loss_cls

        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Train Loss: {total_loss / len(dataloader):.4f}")

#testing loop
def test_loop(dataloader, model):
    model.eval()
    total_loss, total_correct = 0, 0
    total_samples = 0

    with torch.no_grad():
        for batch in dataloader:
            images, ages, age_groups, _ = batch
            images = images.to(device)
            ages = ages.to(device)
            age_groups = age_groups.to(device)

            age_preds, group_preds = model(images)

            loss_reg = regression_loss_fn(age_preds, ages)
            loss_cls = classification_loss_fn(group_preds, age_groups)
            loss = loss_reg + loss_cls
            total_loss += loss.item()

            pred_classes = group_preds.argmax(dim=1)
            total_correct += (pred_classes == age_groups).sum().item()
            total_samples += age_groups.size(0)

    accuracy = 100 * total_correct / total_samples
    avg_loss = total_loss / len(dataloader)
    print(f"Test Loss: {avg_loss:.4f}, Age Group Accuracy: {accuracy:.2f}%")

## Training and Checkpoint Saving <---

#save model function
def save_checkpoint(model, optimizer, epoch, path="age_model_checkpoint.pth"):
    torch.save({
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
    }, path)

#load model function
def load_checkpoint(model, optimizer, path="age_model_checkpoint.pth"):
    if os.path.exists(path):
        checkpoint = torch.load(path)
        model.load_state_dict(checkpoint["model_state_dict"])
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        start_epoch = checkpoint["epoch"] + 1
        print(f"Resuming training from epoch {start_epoch}")
        return start_epoch
    else:
        print("Starting fresh training.")
        return 0

## Run Training

epochs = 10
start_epoch = load_checkpoint(model, optimizer)

for epoch in range(start_epoch, epochs):
    print(f"\nEpoch {epoch+1}/{epochs}")
    train_loop(train_loader, model, optimizer)
    test_loop(test_loader, model)
    save_checkpoint(model, optimizer, epoch)





